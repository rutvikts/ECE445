{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "image1_path = '/Users/nazarkalyniouk/Downloads/personwalking1.jpeg'\n",
    "image2_path = '/Users/nazarkalyniouk/Downloads/personwalking2.jpeg'\n",
    "image3_path = '/Users/nazarkalyniouk/Downloads/personwalking3.webp'\n",
    "image4_path = '/Users/nazarkalyniouk/Downloads/personwalking4.jpeg'\n",
    "image_path_child1 = '/Users/nazarkalyniouk/Downloads/childwalking1.jpeg'\n",
    "im3 = '/Users/nazarkalyniouk/Downloads/cw3.jpeg'\n",
    "im_child = '/Users/nazarkalyniouk/Downloads/child.jpeg'\n",
    "pic = '/Users/nazarkalyniouk/Downloads/pic.webp'\n",
    "pic1 = '/Users/nazarkalyniouk/Downloads/kid.jpeg'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing YOLO if OpenCV doesn't work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "from IPython.display import display, Image\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "net = cv2.dnn.readNet(\"yolov3.weights\", \"yolov3.cfg\")\n",
    "layer_names = net.getLayerNames()\n",
    "output_layers = [layer_names[i[0] - 1] for i in net.getUnconnectedOutLayers()]\n",
    "\n",
    "with open(\"coco.names\", \"r\") as f:\n",
    "    classes = [line.strip() for line in f.readlines()]\n",
    "\n",
    "image = cv2.imread(image1_path)\n",
    "height, width, channels = image.shape\n",
    "\n",
    "blob = cv2.dnn.blobFromImage(image, 0.00392, (416, 416), (0, 0, 0), True, crop=False)\n",
    "\n",
    "net.setInput(blob)\n",
    "outs = net.forward(output_layers)\n",
    "\n",
    "for out in outs:\n",
    "    for detection in out:\n",
    "        scores = detection[5:]\n",
    "        class_id = np.argmax(scores)\n",
    "        confidence = scores[class_id]\n",
    "        if confidence > 0.5 and class_id == 0:  # Class ID 0 corresponds to person in COCO dataset\n",
    "            # Get the bounding box coordinates\n",
    "            center_x = int(detection[0] * width)\n",
    "            center_y = int(detection[1] * height)\n",
    "            w = int(detection[2] * width)\n",
    "            h = int(detection[3] * height)\n",
    "            x = int(center_x - w / 2)\n",
    "            y = int(center_y - h / 2)\n",
    "            cv2.rectangle(image, (x, y), (x + w, y + h), (255, 0, 0), 2)\n",
    "            cv2.putText(image, 'Person', (x, y - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 0, 0), 2)\n",
    "\n",
    "image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "# Display the image in the notebook\n",
    "plt.figure(figsize=(10,10))\n",
    "plt.imshow(image_rgb)\n",
    "plt.axis('off')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing Faster R-CNN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load the pre-trained Faster R-CNN model with ResNet-50 backbone\n",
    "net = cv2.dnn.readNetFromTensorflow(\"faster_rcnn_resnet50_coco.pb\", \"faster_rcnn_resnet50_coco.pbtxt\")\n",
    "\n",
    "# Load the image\n",
    "image = cv2.imread(image1_path)\n",
    "(h, w) = image.shape[:2]\n",
    "\n",
    "# Preprocess the image (resize and normalization)\n",
    "blob = cv2.dnn.blobFromImage(cv2.resize(image, (300, 300)), swapRB=True, crop=False)\n",
    "net.setInput(blob)\n",
    "\n",
    "# Forward pass through the network to perform object detection\n",
    "detections = net.forward()\n",
    "\n",
    "# Loop over the detections\n",
    "for i in range(0, detections.shape[2]):\n",
    "    # Extract the confidence (probability) associated with the prediction\n",
    "    confidence = detections[0, 0, i, 2]\n",
    "\n",
    "    # Filter out weak detections by ensuring the confidence is greater than the minimum confidence threshold\n",
    "    if confidence > 0.5:\n",
    "        # Extract the index of the class label from the detections list\n",
    "        class_id = int(detections[0, 0, i, 1])\n",
    "\n",
    "        # If the detected object is a person\n",
    "        if class_id == 1:\n",
    "            # Extract the bounding box coordinates of the person\n",
    "            box = detections[0, 0, i, 3:7] * np.array([w, h, w, h])\n",
    "            (startX, startY, endX, endY) = box.astype(\"int\")\n",
    "\n",
    "            # Draw the bounding box and label on the image\n",
    "            cv2.rectangle(image, (startX, startY), (endX, endY), (0, 255, 0), 2)\n",
    "            y = startY - 15 if startY - 15 > 15 else startY + 15\n",
    "            cv2.putText(image, \"Person\", (startX, y), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)\n",
    "\n",
    "# Convert the image from BGR to RGB (OpenCV uses BGR by default)\n",
    "image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "# Display the image with detections\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.imshow(image_rgb)\n",
    "plt.axis('off')\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
